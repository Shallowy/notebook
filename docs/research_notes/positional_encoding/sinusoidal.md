# Sinusoidal 位置编码 Sinusoidal Positional Encoding

!!! abstract
    **Sinusoidal 位置编码(Sinusoidal Positional Encoding)** 是一种经典的绝对位置编码方法，最早由 Vaswani 等人在论文 [Attention is All You Need](https://arxiv.org/abs/1706.03762) 中提出，用于 Transformer 模型中。

---

## 1. 绝对位置编码 Absolute Positional Encoding

为序列中的每个位置添加一个唯一的向量表示，使模型能够区分不同位置的信息（即破坏置换不变性）。每个位置的编码是固定的，与输入数据无关。

一般而言，假设有输入序列 $x_1, x_2, ..., x_n$，经过词嵌入得到词向量表示 $a_1, a_2, ..., a_n\in \mathbb{R}^d$, 则位置编码 $e_1, e_2, ..., e_n \in \mathbb{R}^d$ 也具有相同的维度 $d$。最终的输入表示为 $z_i = a_i + e_i$.

???+ question "为什么 $e_i$ 可以直接加到 $a_i$ 上？"
    ??? note "解释0（我认为不太有道理）"
        在 [Transformer介绍](../../cs_notes/nlp/chap7.md/#positional-encoding) 中提到的最简单的位置编码方法是给每个 $x_i$ 拼接一个 one-hot 向量 $p_i$. 
        
        但这样会增加输入的维度，且 one-hot 向量之间是正交的，无法反映位置之间的相对关系，所以并不是一种好的编码方式。

        我们可以这样考虑：上述方法中，在 $x_i$ 上拼接 $p_i$ 后，仍然需要经过一个线性变换 $W$ 才能得到最终的词向量表示。那不妨我们直接令 $x_i$ 也成为 one-hot 的形式（实际情况下可能不是仅有一个1的独热编码，而是多个小数的编码，但含义类似），让它应用线性变换时的含义也变成一种“查表”的过程，这样就和 $p_i$ 的作用类似了。

        <figure markdown="span">
            ![](img/01.jpg){width="500"}
        </figure>

        注意图中不小心把 $x_i$ 和 $p_i$ 画成了相同维度，且都是严格的独热编码，实际情况不一定如此。

        这样一来，我们发现，当 $x_i$ 和 $p_i$ 拼接后，经过线性变换 $W$ 得到的结果,等价于 $x_i$ 经过线性变换 $W_x$, 再加上 $p_i$ 经过线性变换 $W_p$ 的结果，即 $a_i + e_i$ 的结果。

        ---

        事实上在 transformer 中，位置编码是在词嵌入之后才添加的，且它们是单独计算的固定向量，只与位置有关（即上图嵌入矩阵中绿色那部分其实是固定的，并不是可学习的），所以在这里讨论词嵌入矩阵 $W$ 的作用并不严谨。

    ???+ note "解释1（我觉得最好的解释）"
        在注意力机制中，我们在计算词向量 $x$ 和 $y$ 之间的注意力分数时，一个通过线性变换 $W_Q$ 得到查询向量 $Q = W_Q x$，另一个通过线性变换 $W_K$ 得到键向量 $K = W_K y$，然后计算它们的点积 $$ Q \cdot K = (W_Q x) \cdot (W_K y) = x^T W_Q^T W_K y $$

        假设我们对 $x$, $y$ 分别添加位置编码 $e_x, e_y$，则新的注意力分数为

        $$ \begin{aligned} (W_Q (x + e_x)) \cdot (W_K (y + e_y)) &= (x + e_x)^T W_Q^T W_K (y + e_y) \\ &= x^T W_Q^T W_K y + e_x^T W_Q^T W_K y + x^T W_Q^T W_K e_y + e_x^T W_Q^T W_K e_y \end{aligned} $$

        其中第一项是原本的注意力分数，第二、三项分别表示“给定 $e_x$，我需要给 $y$ 多少关注”、“给定 $x$, 我需要给 $e_y$ 多少关注”和“给定 $e_x$， 我需要给 $e_y$ 多少关注”。
        
        所以本质上，通过学习得到的变换矩阵 $W_Q^T W_K$ 需要同时完成上述四项任务。这看似难以权衡，但实际上，存在一个重要的反直观理论：**高维空间中的任意两个向量几乎都是正交的**。而且，**我们没有任何理由认为词向量和位置编码之间有什么特殊的关系**（因为它们是根据各自的机制独立计算的）。因此，很有可能的事实是，词向量和位置编码各自形成了一个较小维度的子空间（这点似乎有研究证实），而这两个子空间之间近似正交，即存在于高维空间中不同的轴上。那么，它们就可以通过同一个学习到的线性变换矩阵 $W_Q^T W_K$ 来分别完成各自的变换，且互不干扰。

        另外，如果上述猜测成立，注意力分数公式中比重较大的项应该是第一项和第四项，即只剩下 $$ x^T W_Q^T W_K y + e_x^T W_Q^T W_K e_y $$

        这似乎可以解释[为什么位置编码之间的点积也是重要的一项](./#_2)，我们将其用于捕捉位置之间的相对关系。

    ???+ note "解释2（其实并不能很好解释问题，只是可以提供一种通用思路）"
        更一般地，我们可以探讨在什么情况下，聚合两个同维度向量时，可以用加法来代替拼接。

        简单观察可以发现，只要满足以下三点：

        1. 向量维度较高；
        2. 两个向量由可学习的自由参数组成；
        3. 聚合结果将继续经过可学习的线性变换；

        !!! example 
            假设我们要连接两个同维度向量 $x$, $y$ 得到 $z = \begin{bmatrix} x \\ y \end{bmatrix}$，然后经过线性变换 $W$ 得到最终结果 
            
            $$ \begin{aligned} r = Wz = W \begin{bmatrix} x \\ y \end{bmatrix} = W_x x + W_y y \end{aligned} $$ 
            
            其中 $W_x, W_y$ 分别是 $W$ 的前半部分和后半部分，即 $W = [W_x, W_y]$.

            那么因为涉及到的所有矩阵和向量都是可学习的，我们可以直接令 $W_x = W_y = W'$, 让 $x$ 和 $y$ 自行调整而不影响表达，那么就有 $$ r = W' x + W' y = W' (x + y) $$


## 2. 整数位置编码

最简单的绝对位置编码方法是直接使用整数索引作为位置编码：

| 位置 | 向量表示 |
| ---- | -------- |
| 0    | [0]      |
| 1    | [1]      |
| 2    | [2]      |
| ...  | ...      |

这种编码方式的主要缺点在于：

- 位置编码的数值范围随序列长度增加而增加，可能导致训练不稳定（一般希望神经网络的权重在0附近，且正负均衡，否则容易导致梯度消失或梯度爆炸）
- 无法推广到训练时未见过的更长序列

## 3. $[0, 1]$ 位置编码

为解决整数位置编码的问题，一种绝对位置编码方法是将位置索引归一化到 $[0, 1]$ 范围内：

| 位置 | 向量表示       |
| ---- | -------------- |
| 0    | $[0.0]$          |
| 1    | $[\frac{1}{n-1}]$      |
| 2    | $[\frac{2}{n-1}]$      |
| ...  | ...            |
| n-1  | $[1.0]$         |

虽然这种编码方式将数值范围固定在了 $[0, 1]$ 内，有助于训练稳定，但它引入了新的缺点：

- 当序列长度变化时，同一位置的编码、位置间的相对距离也会变化，导致模型难以适应不同长度的序列

## 4. 二进制数字位置编码 

另一种绝对位置编码方法是使用二进制数字的向量形式表示位置索引。

例如，假设序列长度不超过16，则每个位置可以用4位二进制数表示：

| 位置 | 二进制 | 向量表示       |
| ---- | ------ | -------------- |
| 0    | 0000   | [0, 0, 0, 0]   |
| 1    | 0001   | [0, 0, 0, 1]   |
| 2    | 0010   | [0, 0, 1, 0]   |
| 3    | 0011   | [0, 0, 1, 1]   |
| 4    | 0100   | [0, 1, 0, 0]   |
| ...  | ...    | ...            |

二进制数字位置编码有一个特点：**低位变化频率高，高位变化频率低。**

虽然这种编码方式使得所有值都有界，且因为token的维度 $d$ 一般足够大，可以表示出序列中所有位置，但它依然存在明显的缺点，例如：

- 位置之间的相对关系难以捕捉（如位置1和位置2的编码差异较小，而位置3和位置4的编码差异较大，但它们在序列中都是相邻的）
- 高维度变化频率过低，如果模型聚焦于学习这些维度的信息，会难以区分（如位置0和位置7的高位部分完全相同）
- 编码出来的位置向量处在一个离散的空间中，难以反映位置之间的连续关系

## 5. 周期性位置编码

考虑到我们希望位置编码以连续的方式逐渐增加（其实这一点并不那么自然，原因有待理解），去插值一个 $0\rightarrow 1\rightarrow 0\rightarrow 1$ 的有界变化趋势，自然会想到使用周期函数，如正弦函数来表示。

为了模拟二进制数字位置编码中低位变化频率高、高位变化频率低的特点，我们可以使用不同频率的正弦函数来表示不同维度的位置编码。具体地，我们可以定义位置 $m$ 的位置编码（设维度为 $d$ ）

$$ \begin{aligned} {PE}_m &= [\sin{m\omega_0}, \sin{m\omega_1}, \sin{m\omega_2}, ..., \sin{m\omega_{d-1}}] \\ &= [\sin{m\frac{\frac{\pi}{2}}{2^0}}, \sin{m\frac{\frac{\pi}{2}}{2^1}}, \sin{m\frac{\frac{\pi}{2}}{2^2}}, ..., \sin{m\frac{\frac{\pi}{2}}{2^{d - 1}}}] \end{aligned} $$

这样就可以最大程度地模拟二进制数字位置编码（第 $i$ 维在第 $2^i$ 个位置第一次由 $0$ 变成 $1$）。

值得注意的是，这样的表示对频率的值提出了要求：如果频率太小，会导致位置编码的数值过小（接近0）；而如果频率太大，因为正弦函数位置编码理论上是一个如下图所示的周期性的环，所以会导致远距离的两个点反而在位置编码上很接近。

<figure markdown="span">
     ![](img/02.jpg){width="300"}
</figure>

为了解决这一问题，我们发现频率 $\omega_0, \omega_1, ..., \omega_{d - 1}$ 并不一定要满足模拟二进制增长，而只要满足**单调递减**即可，这样就可以设计一个适当的频率范围。

???+ note 
    Vaswani 等人选择的值大约为 $\omega_i = \frac{1}{10000^{\frac{i}{d}}}$，具体原因未知，应该是一个实验性的选择。

    实际使用时可以将其设置为一个可训练的参数。

## 6. Sinusoidal 位置编码 Sinusoidal Positional Encoding

### 捕捉相对位置关系

回顾以上提到的各种绝对位置编码的缺点，我们发现还有一个问题没有解决：**捕捉位置之间的相对关系**。

由上文的例子，我们倾向于希望位置1和位置2的关系，与位置3和位置4的关系是类似的甚至相同的。而如何定义“位置之间的关系”呢？普遍意义上，transformer使用了**线性变换**来生成查询、键和值，这就是一种广义上的“关系”。所以，如果我们也能够使用线性变换来“移动”位置编码，就非常理想了，即：

$$ {PE}_{m+k} = T_k \cdot {PE}_m $$

其中 $T_k$ 是一个与位置偏移 $k$ 相关的线性变换矩阵。

在三角函数中，平移等价于相位的变化，对应于**旋转变换**这一类线性变换，所以我们可以利用恒等式

$$ \begin{bmatrix} \sin(x + \phi) \\ \cos(x + \phi) \end{bmatrix} = \begin{bmatrix} \cos(\phi) & \sin(\phi) \\ -\sin(\phi) & \cos(\phi) \end{bmatrix} \begin{bmatrix} \sin(x) \\ \cos(x) \end{bmatrix} $$

来设计位置编码上的线性变换。因此，我们尝试将位置编码的每两个维度作为一组，使用相同频率的正余弦函数来表示：

$$ \begin{aligned} {PE}_m = [ \sin{(m\omega_0)}, \cos{(m\omega_0)}, \sin{(m\omega_1)}, \cos{(m\omega_1)}, ..., \sin{(m\omega_{\frac{d}{2}-1})}, \cos{(m\omega_{\frac{d}{2}-1})} ] \end{aligned} $$

这样一来，我们就可以定义线性变换

$$ T_k = \begin{bmatrix} R(k\omega_0) & 0 & ... & 0 \\ 0 & R(k\omega_1) & ... & 0 \\ ... & ... & ... & ... \\ 0 & 0 & ... & R(k\omega_{\frac{d}{2}-1}) \end{bmatrix} $$

其中 $R(\theta) = \begin{bmatrix} \cos(\theta) & \sin(\theta) \\ -\sin(\theta) & \cos(\theta) \end{bmatrix}$.

### 位置编码定义

Sinusoidal 位置编码的正式定义为：

$$ \begin{aligned} {PE}_{pos, 2i} &= \sin{\left(pos \cdot \frac{1}{10000^{\frac{2i}{d}}}\right)} \\ {PE}_{pos, 2i+1} &= \cos{\left(pos \cdot \frac{1}{10000^{\frac{2i}{d}}}\right)} \end{aligned} $$

其中 $pos$ 是位置索引，$i$ 是维度索引，$d$ 是位置编码的总维度。 

???+ note "Sinusoidal 位置编码的一点性质"

    容易证明，两不同位置的编码之间的点积只与它们的相对位置有关，且具有远程衰减的特点，且无方向性。

    暂时不知道这一性质是否有实际意义，因为位置编码的数值相对于词向量来说较小，所以点积的值也会较小。目前猜测是因为在计算注意力时，位置编码还要经过 $W_Q, W_K$ 的线性变换，重新调整分布，以此影响注意力的计算。

    <figure markdown="span">
        ![](img/03.jpg){width="400"}
    </figure>

    另外，这个性质或许可以解释二进制数字编码中“高维度变化频率过低，如果模型聚焦于学习这些维度的信息，会难以区分”的问题：**模型可以利用高频的维度来捕捉短距离信息，而利用低频的维度来捕捉长距离信息**。

    <figure markdown="span">
        ![](img/04.jpg){width="400"}
    </figure>

<!-- ## 4. Sinusoidal 位置编码 Sinusoidal Positional Encoding

我们希望设计出的位置编码能够打破置换不变性，且最好反映位置之间的相对关系。接下来我们考虑形式化地表达这两点。

### 3.1 打破置换不变性

假设我们的模型是一个函数 $f(a_1, a_2, ..., a_n)$，本来由于置换不变性，有 $$ f(..., a_m, ..., a_n, ...) = f(..., a_n, ..., a_m, ...) $$

为了打破置换不变性，我们引入位置编码 $e_1, e_2, ..., e_n$，希望使得 $$ f(..., a_m + e_m, ..., a_n + e_n, ...) \neq f(..., a_n + e_m, ..., a_m + e_n, ...) $$

因为位置编码传递的位置信息一般少于词向量传递的含义信息，所以我们可以尝试对上式进行泰勒展开，依旧仅考虑 $m, n$ 两个位置，得到

$$ \begin{aligned} &\ f(a_m + e_m, a_n + e_n) \\ =&\ f(a_m, a_n) + \frac{\partial f}{\partial a_m} e_m + \frac{\partial f}{\partial a_n} e_n + \frac{1}{2} \frac{\partial^2 f}{\partial a_m^2} e_m^2 + \frac{1}{2} \frac{\partial^2 f}{\partial a_n^2} e_n^2 + \frac{\partial^2 f}{\partial a_m \partial a_n} e_m \cdot e_n + ... \end{aligned} $$

展开式中第1项和位置无关，第2到5项仅和单一位置有关，只有第6项包含了两个位置之间的关系。

### 3.2 表达相对关系

由上，我们希望展开式中 $ \frac{\partial^2 f}{\partial a_m \partial a_n} e_m \cdot e_n $ 这项能够表达位置之间的相对关系，即存在某函数 $g$ 使得 $$ \frac{\partial^2 f}{\partial a_m \partial a_n} e_m \cdot e_n = g(m - n) $$ -->